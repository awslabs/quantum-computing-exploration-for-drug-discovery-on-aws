{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0531faa8-9b68-4a7f-833a-b18efbac1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.aws import AwsDevice\n",
    "from braket.aws import AwsQuantumJob, AwsSession\n",
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6283c75a-3c8b-4a9e-9fd8-709e7d8da56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "c = copy.deepcopy\n",
    "\n",
    "def get_key(single_dict):\n",
    "    for k in single_dict.keys():\n",
    "        return k\n",
    "\n",
    "def parse_params(params_list, hp, hp_list):\n",
    "    params = params_list[0]\n",
    "    k = get_key(params)\n",
    "    ps = params[k]\n",
    "    for p in ps:\n",
    "        hp[k] = p\n",
    "        if len(params_list) == 1:\n",
    "            hp_list.append(c(hp))\n",
    "        else:\n",
    "            parse_params(params_list[1:], hp, hp_list)\n",
    "\n",
    "def get_quantum_device(device_name):\n",
    "    device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "    try:\n",
    "        device = AwsDevice.get_devices(names=[device_name])\n",
    "        device_arn = device[0].arn\n",
    "    except Exception as e:\n",
    "        print(f\"fail to get {device_name}: {e}, use sv1 instead\")\n",
    "    return device_arn\n",
    "\n",
    "def upload_data(dir_name, suffix_check, aws_session=AwsSession()):\n",
    "    stream_s3_uri = aws_session.construct_s3_uri(aws_session.default_bucket(), dir_name)\n",
    "    return_path = None\n",
    "    \n",
    "    def _check_upload(file_name, check_list):\n",
    "        file_end = file_name.split('.')[-1]\n",
    "        if file_end in check_list:\n",
    "            path = f\"{stream_s3_uri}/\" + file_name.split('/')[-1]\n",
    "            aws_session.upload_to_s3(file_name, path)\n",
    "            \n",
    "    if os.path.isdir(dir_name):\n",
    "        dir_list = os.listdir(dir_name)\n",
    "        for file_name in dir_list:\n",
    "            _check_upload(os.path.join(dir_name,file_name), suffix_check)\n",
    "        return_path = stream_s3_uri\n",
    "    else:\n",
    "        _check_upload(file_name, suffix_check)\n",
    "        single_file_name = file_name.split('/')[-1]\n",
    "        return_path = f\"{stream_s3_uri}/{single_file_name}\"\n",
    "        \n",
    "    return return_path\n",
    "\n",
    "def queue_check(jobs):\n",
    "    queue_count = 0\n",
    "    running_count = 0\n",
    "    check_pass = True\n",
    "    for job in jobs:\n",
    "        # print(f\"job state {job.state()}\")\n",
    "        if job.state() == \"QUEUED\":\n",
    "            queue_count = queue_count + 1\n",
    "        if job.state() == \"RUNNING\":\n",
    "            running_count = running_count + 1\n",
    "        if queue_count == 4 or running_count == 4:\n",
    "            check_pass = False\n",
    "    \n",
    "    print(f\"queue_count {queue_count}, running_count {running_count}\")\n",
    "    \n",
    "    return check_pass\n",
    "\n",
    "def get_result(result, target, dm):\n",
    "    return [dm, result[\"time\"], target]\n",
    "\n",
    "def get_dm(target):\n",
    "    file_name = './test-folding/' + target + '.fasta.txt'\n",
    "    str_len = -100\n",
    "    with open(file_name) as file:\n",
    "        fasta_lines = file.readlines()\n",
    "        str_len = len(fasta_lines[1])\n",
    "    return str_len\n",
    "\n",
    "def display_results(results, experiments_params):\n",
    "    sorted_results = {}\n",
    "    \n",
    "    for device in experiments_params[\"params\"][4][\"device\"]:\n",
    "        sorted_results[str(device)] = []\n",
    "        # for target in results[0].keys():\n",
    "        #     sorted_results[str(device)][target] = []\n",
    "    \n",
    "    max_offset = 10\n",
    "    for result in results:\n",
    "        for target in result.keys():\n",
    "            device = result[target][\"hypermeter\"][\"device\"]\n",
    "            \n",
    "            dm = get_dm(target)\n",
    "\n",
    "            if len(sorted_results[device]) == 0:\n",
    "                sorted_results[device].append(get_result(result[target],target,dm))\n",
    "                continue\n",
    "\n",
    "            last_idx = 0\n",
    "            for idx, sorted_result in enumerate(sorted_results[device]):\n",
    "                sorted_dm = float(sorted_result[0])\n",
    "\n",
    "                last_sorted_dm = float(sorted_results[device][last_idx][0])\n",
    "\n",
    "                if last_sorted_dm < dm and dm < sorted_dm:\n",
    "                    sorted_results[device].insert(idx,get_result(result[target],target,dm))\n",
    "                    break\n",
    "                elif dm < last_sorted_dm and idx == 0:\n",
    "                    sorted_results[device].insert(last_idx,get_result(result[target],target,dm))\n",
    "                    break\n",
    "                elif dm > sorted_dm and idx == len(sorted_results[device])-1:\n",
    "                    sorted_results[device].insert(idx+1,get_result(result[target],target,dm))\n",
    "\n",
    "                last_idx = idx\n",
    "    # print(f\"sorted result {sorted_results}\")\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3f2cc57f-b61f-48ef-8d7b-580f47f66d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for experiments: \n",
      " [{'PKP': -1.0, 'S': 1, 'O': 1000000, 'shots': 10000, 'device': {'qc': 'null', 'cc': 'ml.m5.large'}}, {'PKP': -1.0, 'S': 1, 'O': 1000000, 'shots': 10000, 'device': {'qc': 'null', 'cc': 'ml.m5.4xlarge'}}]\n"
     ]
    }
   ],
   "source": [
    "# parameters for experiments\n",
    "experiment_name = \"rna-folding\"\n",
    "data_path = \"rna-folding-data\"\n",
    "suffix_check = [\"txt\"]\n",
    "experiments_params =  {\n",
    "    \"version\": \"1\",\n",
    "    \"params\": [\n",
    "        # {\"PKP\": [-1.0, -0.5, 0.0, 0.5, 1.0]},\n",
    "        {\"PKP\": [-1.0]},\n",
    "        {\"S\": [1]},\n",
    "        {\"O\": [1000000]},\n",
    "        {\"shots\": [10000]},\n",
    "        {\"device\": [{\"qc\": \"null\", \"cc\": \"ml.m5.large\"},{\"qc\": \"null\", \"cc\": \"ml.m5.4xlarge\"}]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "hp = {}\n",
    "hybrid_job_params = []\n",
    "parse_params(experiments_params['params'], hp, hybrid_job_params)\n",
    "\n",
    "print(f\"parameters for experiments: \\n {hybrid_job_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b2062d7b-b958-4c35-bf9f-202480d4d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload data to s3 path: s3://amazon-braket-us-east-1-002224604296/rna-folding-data\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to S3\n",
    "s3_path = upload_data(data_path, suffix_check)\n",
    "print(f\"upload data to s3 path: {s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "320b1c6b-c575-4d41-a69a-cde747d903cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hybrid job image for 002224604296 in region us-east-1: 002224604296.dkr.ecr.us-east-1.amazonaws.com/amazon-braket-rna-folding-jobs:latest\n"
     ]
    }
   ],
   "source": [
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.client('s3').meta.region_name\n",
    "image_name = f\"amazon-braket-{experiment_name}-jobs\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "\n",
    "print(f\"the hybrid job image for {account_id} in region {region}: {image_uri}\")\n",
    "\n",
    "# # Uncomment the following code if you want to rebuild the container\n",
    "# !sh build_and_push.sh {image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "87c7a433-6518-4f13-8b0f-4abd35b4c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job info will be saved in rna-folding-hybrid-jobs.json\n"
     ]
    }
   ],
   "source": [
    "hybrid_jobs_json = f\"{experiment_name}-hybrid-jobs.json\"\n",
    "print(f\"job info will be saved in {hybrid_jobs_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1d52f18-791a-4943-abaf-1a9911815709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'rna-folding-hybrid-jobs.json': No such file or directory\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is rna-folding-PKP--10-ml-m5-large-1673244331\n",
      "Finish create rna-folding with PKP -1.0, S 1 , O 1000000 and device ml-m5-large\n",
      "queue_count 1, running_count 0\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is rna-folding-PKP--10-ml-m5-4large-1673244336\n",
      "Finish create rna-folding with PKP -1.0, S 1 , O 1000000 and device ml-m5-4large\n",
      "queue_count 1, running_count 1\n",
      "Finish launch all the hybrid jobs and save all the files\n"
     ]
    }
   ],
   "source": [
    "# Long runnning cell due to Burst rate of CreateJob requests < 5 RPS\n",
    "# sudo apt-get install python-prctl at first\n",
    "# https://stackoverflow.com/questions/34361035/python-thread-name-doesnt-show-up-on-ps-or-htop\n",
    "from threading import Thread\n",
    "import threading\n",
    "import setproctitle\n",
    "\n",
    "def launch_hybrid_jobs(hybrid_job_params=hybrid_job_params, hybrid_jobs_json=hybrid_jobs_json):\n",
    "    setproctitle.setproctitle(threading.currentThread().name)\n",
    "    # parse evaluation parameters and trigger hybrid jobs:\n",
    "    jobs = []\n",
    "    names = []\n",
    "\n",
    "    job_name = f\"{experiment_name}-job\"\n",
    "\n",
    "    for job_param in hybrid_job_params:\n",
    "        PKP = job_param['PKP']\n",
    "        S = job_param['S']\n",
    "        O = job_param['O']\n",
    "        quantum_device = get_quantum_device(job_param['device']['qc'])\n",
    "        classical_device = job_param['device']['cc']\n",
    "\n",
    "        device_name = classical_device.replace(\".\",\"-\")\n",
    "        device_name = device_name.replace(\"x\",\"\")\n",
    "        \n",
    "        name = f\"{experiment_name}-PKP-{str(PKP).replace('.','')}-{device_name}-\" + str(int(time.time()))\n",
    "        print(f\"name is {name}\")\n",
    "\n",
    "        tmp_job = AwsQuantumJob.create(\n",
    "            device=quantum_device,\n",
    "            source_module=f\"{experiment_name}\",\n",
    "            entry_point=f\"{experiment_name}.{job_name}:main\",\n",
    "            job_name=name,\n",
    "            hyperparameters=job_param,\n",
    "            input_data=s3_path,\n",
    "            instance_config=InstanceConfig(instanceType=classical_device),\n",
    "            image_uri=image_uri,\n",
    "            wait_until_complete=False,\n",
    "        )\n",
    "        \n",
    "#         from braket.jobs.local import LocalQuantumJob\n",
    "        \n",
    "#         tmp_job = LocalQuantumJob.create(\n",
    "#             device=quantum_device,\n",
    "#             source_module=f\"{experiment_name}\",\n",
    "#             entry_point=f\"{experiment_name}.{job_name}:main\",\n",
    "#             hyperparameters=job_param,\n",
    "#             input_data=s3_path,\n",
    "#             image_uri=image_uri,\n",
    "#         )   \n",
    "        \n",
    "        \n",
    "        print(f\"Finish create {experiment_name} with PKP {PKP}, S {S} , O {O} and device {device_name}\")\n",
    "\n",
    "        jobs.append(tmp_job)\n",
    "        names.append(name)\n",
    "\n",
    "\n",
    "        while not queue_check(jobs):\n",
    "            time.sleep(5)\n",
    "    jobs_arn = []\n",
    "\n",
    "    for job in jobs:\n",
    "        jobs_arn.append(job.arn)\n",
    "\n",
    "    jobs_states = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"hybrid-jobs-arn\": jobs_arn,\n",
    "        \"names\": names\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # save hybrid job arn for further analysis\n",
    "    json_object = json.dumps(jobs_states, indent=4)\n",
    "\n",
    "    with open(hybrid_jobs_json, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "        \n",
    "    print(f\"Finish launch all the hybrid jobs and save all the files\")\n",
    "\n",
    "# remove existing hybrid_jobs_json file\n",
    "!rm {hybrid_jobs_json}\n",
    "\n",
    "t = Thread(target=launch_hybrid_jobs, name=\"launch-hybrid-job\", daemon=True).start()\n",
    "\n",
    "# launch_hybrid_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9d1b433d-440a-42ed-8cf2-fc6b48ea97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu     85131  0.0  1.3 1667824 438060 ?      Ssl  Jan05   0:25 launch-hybrid-job\n",
      "ubuntu     90748  0.0  1.5 1867392 494316 ?      Ssl  Jan06   1:30 launch-hybrid-job\n",
      "ubuntu    132026  0.0  0.0   8748  3300 pts/0    Ss+  06:14   0:00 /bin/bash -c ps -aux | grep launch-hybrid-job\n",
      "ubuntu    132028  0.0  0.0   8176   724 pts/0    S+   06:14   0:00 grep launch-hybrid-job\n"
     ]
    }
   ],
   "source": [
    "# run the following scripts to check the created threads\n",
    "!ps -aux | grep launch-hybrid-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a643773a-ecaa-4af5-a7eb-ed276f7930b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state of job rna-folding-PKP--10-ml-m5-large-1673244331 is : RUNNING\n",
      "the state of job rna-folding-PKP--10-ml-m5-4large-1673244336 is : RUNNING\n"
     ]
    }
   ],
   "source": [
    "# run the following code to test whether all the jobs finish\n",
    "results = []\n",
    "if os.path.exists(hybrid_jobs_json):\n",
    "    # recover hybrid jobs and show result\n",
    "    jobs_states_load = None\n",
    "    with open(hybrid_jobs_json, \"r\") as outfile:\n",
    "        jobs_states_load = json.load(outfile)\n",
    "\n",
    "    completed_jobs_arn = set()\n",
    "\n",
    "    for job_name, job_arn in zip(jobs_states_load[\"names\"], jobs_states_load[\"hybrid-jobs-arn\"]):\n",
    "        current_job = AwsQuantumJob(job_arn)\n",
    "        print(f\"the state of job {job_name} is : {current_job.state()}\")\n",
    "        if current_job.state() == 'COMPLETED':\n",
    "            completed_jobs_arn.update({job_arn})\n",
    "\n",
    "    whole_jobs_num = len(jobs_states_load[\"names\"])\n",
    "\n",
    "    if len(completed_jobs_arn) == whole_jobs_num:\n",
    "        print(f\"all jobs completed\")\n",
    "        for job_arn in completed_jobs_arn:\n",
    "            current_job = AwsQuantumJob(job_arn)\n",
    "            results.append(current_job.result())\n",
    "        # display results\n",
    "        results = display_results(results, experiments_params)\n",
    "else:\n",
    "    print(f\"JSON file for job arns not generated! please wait for the thread(launch-hybrid-job) to finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9924cb0d-be27-49be-a8f0-e466648e269a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [136], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m y_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,vs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m      6\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     dict_k \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(k)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "rename_result = {}\n",
    "device_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "for k,vs in results.items():\n",
    "    k = k.replace(\"\\'\",\"\\\"\")\n",
    "    dict_k = json.loads(k)\n",
    "    device_name = None\n",
    "    if dict_k['qc'] == 'null':\n",
    "        device_name = dict_k['cc']\n",
    "    else:\n",
    "        device_name = dict_k['qc']\n",
    "    for v in vs:\n",
    "        device_list.append(device_name)\n",
    "        x_list.append(v[0])\n",
    "        y_list.append(v[1])\n",
    "source = pd.DataFrame({\n",
    "    \"Sequence Length\": np.array(x_list),\n",
    "    \"Time to Solution\": np.array(y_list),\n",
    "    \"Device\": np.array(device_list),\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_line(point = True).encode(\n",
    "    x='Sequence Length',\n",
    "    y='Time to Solution',\n",
    "    color='Device',\n",
    ").properties(\n",
    "    title = f\"{experiment_name} experiments\",\n",
    "    width = 700,\n",
    "    height = 600,\n",
    ").interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

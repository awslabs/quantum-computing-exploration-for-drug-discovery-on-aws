{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0531faa8-9b68-4a7f-833a-b18efbac1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.aws import AwsDevice\n",
    "from braket.aws import AwsQuantumJob, AwsSession\n",
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6283c75a-3c8b-4a9e-9fd8-709e7d8da56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "c = copy.deepcopy\n",
    "\n",
    "def obtain_default_bucket(target: str) -> str:\n",
    "    with open(os.path.abspath('../..') + '/.default-setting') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if (line.startswith(target+'=')):\n",
    "                return line.split('=')[1].strip()\n",
    "            \n",
    "def get_key(single_dict):\n",
    "    for k in single_dict.keys():\n",
    "        return k\n",
    "\n",
    "def parse_params(params_list, hp, hp_list):\n",
    "    params = params_list[0]\n",
    "    k = get_key(params)\n",
    "    ps = params[k]\n",
    "    for p in ps:\n",
    "        hp[k] = p\n",
    "        if len(params_list) == 1:\n",
    "            hp_list.append(c(hp))\n",
    "        else:\n",
    "            parse_params(params_list[1:], hp, hp_list)\n",
    "\n",
    "def get_quantum_device(device_name):\n",
    "    device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "    try:\n",
    "        device = AwsDevice.get_devices(names=[device_name])\n",
    "        device_arn = device[0].arn\n",
    "    except Exception as e:\n",
    "        print(f\"fail to get {device_name}: {e}, use sv1 instead\")\n",
    "    return device_arn\n",
    "\n",
    "def upload_data(dir_name, suffix_check, aws_session=AwsSession()):\n",
    "    stream_s3_uri = aws_session.construct_s3_uri(obtain_default_bucket(\"bucketName\"), dir_name)\n",
    "    return_path = None\n",
    "    \n",
    "    def _check_upload(file_name, check_list):\n",
    "        file_end = file_name.split('.')[-1]\n",
    "        if file_end in check_list:\n",
    "            path = f\"{stream_s3_uri}/\" + file_name.split('/')[-1]\n",
    "            aws_session.upload_to_s3(file_name, path)\n",
    "            \n",
    "    if os.path.isdir(dir_name):\n",
    "        dir_list = os.listdir(dir_name)\n",
    "        for file_name in dir_list:\n",
    "            _check_upload(os.path.join(dir_name,file_name), suffix_check)\n",
    "        return_path = stream_s3_uri\n",
    "    else:\n",
    "        _check_upload(file_name, suffix_check)\n",
    "        single_file_name = file_name.split('/')[-1]\n",
    "        return_path = f\"{stream_s3_uri}/{single_file_name}\"\n",
    "        \n",
    "    return return_path\n",
    "\n",
    "def queue_check(jobs):\n",
    "    queue_count = 0\n",
    "    running_count = 0\n",
    "    check_pass = True\n",
    "    for job in jobs:\n",
    "        # print(f\"job state {job.state()}\")\n",
    "        if job.state() == \"QUEUED\":\n",
    "            queue_count = queue_count + 1\n",
    "        if job.state() == \"RUNNING\":\n",
    "            running_count = running_count + 1\n",
    "        if queue_count == 4 or running_count == 4:\n",
    "            check_pass = False\n",
    "    \n",
    "    print(f\"queue_count {queue_count}, running_count {running_count}\")\n",
    "    \n",
    "    return check_pass\n",
    "\n",
    "def get_result(result, target, dm):\n",
    "    return [dm, result[\"time\"], target]\n",
    "\n",
    "def get_dm(target):\n",
    "    file_name = './rna-folding-data/' + target + '.fasta.txt'\n",
    "    str_len = -100\n",
    "    with open(file_name) as file:\n",
    "        fasta_lines = file.readlines()\n",
    "        str_len = len(fasta_lines[1])\n",
    "    return str_len\n",
    "\n",
    "def display_results(results, experiments_params):\n",
    "    sorted_results = {}\n",
    "    \n",
    "    for device in experiments_params[\"params\"][4][\"device\"]:\n",
    "        sorted_results[str(device)] = []\n",
    "        # for target in results[0].keys():\n",
    "        #     sorted_results[str(device)][target] = []\n",
    "    \n",
    "    max_offset = 10\n",
    "    for result in results:\n",
    "        for target in result.keys():\n",
    "            device = result[target][\"hypermeter\"][\"device\"]\n",
    "            \n",
    "            dm = get_dm(target)\n",
    "\n",
    "            if len(sorted_results[device]) == 0:\n",
    "                sorted_results[device].append(get_result(result[target],target,dm))\n",
    "                continue\n",
    "\n",
    "            last_idx = 0\n",
    "            for idx, sorted_result in enumerate(sorted_results[device]):\n",
    "                sorted_dm = float(sorted_result[0])\n",
    "\n",
    "                last_sorted_dm = float(sorted_results[device][last_idx][0])\n",
    "\n",
    "                if last_sorted_dm < dm and dm < sorted_dm:\n",
    "                    sorted_results[device].insert(idx,get_result(result[target],target,dm))\n",
    "                    break\n",
    "                elif dm < last_sorted_dm and idx == 0:\n",
    "                    sorted_results[device].insert(last_idx,get_result(result[target],target,dm))\n",
    "                    break\n",
    "                elif dm > sorted_dm and idx == len(sorted_results[device])-1:\n",
    "                    sorted_results[device].insert(idx+1,get_result(result[target],target,dm))\n",
    "\n",
    "                last_idx = idx\n",
    "    # print(f\"sorted result {sorted_results}\")\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69851056-a6fb-46f9-9915-4ebdd0b300d9",
   "metadata": {},
   "source": [
    "# Step 1: Prepare parameters for batch evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933ed89-1906-4b4f-9d5e-b6af78a28cea",
   "metadata": {},
   "source": [
    "In this part, we set the parameters for batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3f2cc57f-b61f-48ef-8d7b-580f47f66d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for experiments: \n",
      " [{'PKP': -1.0, 'S': 1, 'O': 1000000, 'shots': 10000, 'device': {'qc': 'null', 'cc': 'ml.m5.large'}}, {'PKP': -1.0, 'S': 1, 'O': 1000000, 'shots': 10000, 'device': {'qc': 'null', 'cc': 'ml.m5.4xlarge'}}]\n"
     ]
    }
   ],
   "source": [
    "# parameters for experiments\n",
    "experiment_name = \"rna-folding\"\n",
    "data_path = \"rna-folding-data\"\n",
    "suffix_check = [\"txt\"]\n",
    "experiments_params =  {\n",
    "    \"version\": \"1\",\n",
    "    \"params\": [\n",
    "        # {\"PKP\": [-1.0, -0.5, 0.0, 0.5, 1.0]},\n",
    "        {\"PKP\": [-1.0]},\n",
    "        {\"S\": [1]},\n",
    "        {\"O\": [1000000]},\n",
    "        {\"shots\": [10000]},\n",
    "        {\"device\": [{\"qc\": \"null\", \"cc\": \"ml.m5.large\"},{\"qc\": \"null\", \"cc\": \"ml.m5.4xlarge\"}]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "hp = {}\n",
    "hybrid_job_params = []\n",
    "parse_params(experiments_params['params'], hp, hybrid_job_params)\n",
    "\n",
    "print(f\"parameters for experiments: \\n {hybrid_job_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b2062d7b-b958-4c35-bf9f-202480d4d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload data to s3 path: s3://amazon-braket-us-east-1-002224604296/rna-folding-data\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to S3\n",
    "s3_path = upload_data(data_path, suffix_check)\n",
    "print(f\"upload data to s3 path: {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed41e7-fbb4-4c16-b375-e7ef94754e12",
   "metadata": {},
   "source": [
    "# Step 2: Prepare image for experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabecc36-a574-4a7c-b9bf-94af53dcb086",
   "metadata": {},
   "source": [
    "In this part, we use the following code to prepare the image for experiment. For the first run, \n",
    "please run build_and_push.sh to create the image. For future experiments, avoid running\n",
    "build_and_push.sh unless you want to rebuild the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "320b1c6b-c575-4d41-a69a-cde747d903cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hybrid job image for 002224604296 in region us-east-1: 002224604296.dkr.ecr.us-east-1.amazonaws.com/amazon-braket-rna-folding-jobs:latest\n"
     ]
    }
   ],
   "source": [
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.client('s3').meta.region_name\n",
    "image_name = f\"amazon-braket-{experiment_name}-jobs\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "\n",
    "print(f\"the hybrid job image for {account_id} in region {region}: {image_uri}\")\n",
    "\n",
    "# For the first run, please use the following code to create the image for this application. For future experiments, comment\n",
    "# the following code unless you want to rebuild the image\n",
    "!sh build_and_push.sh {image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "87c7a433-6518-4f13-8b0f-4abd35b4c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job info will be saved in rna-folding-hybrid-jobs.json\n"
     ]
    }
   ],
   "source": [
    "hybrid_jobs_json = f\"{experiment_name}-hybrid-jobs.json\"\n",
    "print(f\"job info will be saved in {hybrid_jobs_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4c8d9-8db8-443e-992f-bf344b051eff",
   "metadata": {},
   "source": [
    "# Step 3: Launch Amazon Braket Hybrid Jobs for experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a1eb3-bb7f-4f53-804e-fec3db66c1bc",
   "metadata": {},
   "source": [
    "### **Please make sure no hybrid jobs are running in the account before running the following code**\n",
    "\n",
    "In this part, we use the following code to launch the same number of hybrid jobs as the sets of parameters for this experiments.\n",
    "When the number of jobs exceeds 5 RPS, this thread will wait. The default setting of this experiment will take around **7 hours** to \n",
    "finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1d52f18-791a-4943-abaf-1a9911815709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'rna-folding-hybrid-jobs.json': No such file or directory\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is rna-folding-PKP--10-ml-m5-large-1673244331\n",
      "Finish create rna-folding with PKP -1.0, S 1 , O 1000000 and device ml-m5-large\n",
      "queue_count 1, running_count 0\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is rna-folding-PKP--10-ml-m5-4large-1673244336\n",
      "Finish create rna-folding with PKP -1.0, S 1 , O 1000000 and device ml-m5-4large\n",
      "queue_count 1, running_count 1\n",
      "Finish launch all the hybrid jobs and save all the files\n"
     ]
    }
   ],
   "source": [
    "# Long runnning cell due to Burst rate of CreateJob requests < 5 RPS\n",
    "# sudo apt-get install python-prctl at first\n",
    "# https://stackoverflow.com/questions/34361035/python-thread-name-doesnt-show-up-on-ps-or-htop\n",
    "from threading import Thread\n",
    "import threading\n",
    "import setproctitle\n",
    "\n",
    "def launch_hybrid_jobs(hybrid_job_params=hybrid_job_params, hybrid_jobs_json=hybrid_jobs_json):\n",
    "    setproctitle.setproctitle(threading.current_thread().name)\n",
    "    # parse evaluation parameters and trigger hybrid jobs:\n",
    "    jobs = []\n",
    "    names = []\n",
    "\n",
    "    job_name = f\"{experiment_name}-job\"\n",
    "\n",
    "    for job_param in hybrid_job_params:\n",
    "        PKP = job_param['PKP']\n",
    "        S = job_param['S']\n",
    "        O = job_param['O']\n",
    "        quantum_device = get_quantum_device(job_param['device']['qc'])\n",
    "        classical_device = job_param['device']['cc']\n",
    "\n",
    "        device_name = classical_device.replace(\".\",\"-\")\n",
    "        device_name = device_name.replace(\"x\",\"\")\n",
    "        \n",
    "        name = f\"{experiment_name}-PKP-{str(PKP).replace('.','')}-{device_name}-\" + str(int(time.time()))\n",
    "        print(f\"name is {name}\")\n",
    "\n",
    "        tmp_job = AwsQuantumJob.create(\n",
    "            device=quantum_device,\n",
    "            source_module=f\"{experiment_name}\",\n",
    "            entry_point=f\"{experiment_name}.{job_name}:main\",\n",
    "            job_name=name,\n",
    "            hyperparameters=job_param,\n",
    "            input_data=s3_path,\n",
    "            instance_config=InstanceConfig(instanceType=classical_device),\n",
    "            image_uri=image_uri,\n",
    "            wait_until_complete=False,\n",
    "        )\n",
    "        \n",
    "#         from braket.jobs.local import LocalQuantumJob\n",
    "        \n",
    "#         tmp_job = LocalQuantumJob.create(\n",
    "#             device=quantum_device,\n",
    "#             source_module=f\"{experiment_name}\",\n",
    "#             entry_point=f\"{experiment_name}.{job_name}:main\",\n",
    "#             hyperparameters=job_param,\n",
    "#             input_data=s3_path,\n",
    "#             image_uri=image_uri,\n",
    "#         )   \n",
    "        \n",
    "        \n",
    "        print(f\"Finish create {experiment_name} with PKP {PKP}, S {S} , O {O} and device {device_name}\")\n",
    "\n",
    "        jobs.append(tmp_job)\n",
    "        names.append(name)\n",
    "\n",
    "\n",
    "        while not queue_check(jobs):\n",
    "            time.sleep(5)\n",
    "    jobs_arn = []\n",
    "\n",
    "    for job in jobs:\n",
    "        jobs_arn.append(job.arn)\n",
    "\n",
    "    jobs_states = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"hybrid-jobs-arn\": jobs_arn,\n",
    "        \"names\": names\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # save hybrid job arn for further analysis\n",
    "    json_object = json.dumps(jobs_states, indent=4)\n",
    "\n",
    "    with open(hybrid_jobs_json, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "        \n",
    "    print(f\"Finish launch all the hybrid jobs and save all the files\")\n",
    "\n",
    "# remove existing hybrid_jobs_json file\n",
    "!rm {hybrid_jobs_json}\n",
    "\n",
    "t = Thread(target=launch_hybrid_jobs, name=\"launch-hybrid-job\", daemon=True).start()\n",
    "\n",
    "# launch_hybrid_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9d1b433d-440a-42ed-8cf2-fc6b48ea97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu     85131  0.0  1.3 1667824 438060 ?      Ssl  Jan05   0:25 launch-hybrid-job\n",
      "ubuntu     90748  0.0  1.5 1867392 494316 ?      Ssl  Jan06   1:30 launch-hybrid-job\n",
      "ubuntu    132026  0.0  0.0   8748  3300 pts/0    Ss+  06:14   0:00 /bin/bash -c ps -aux | grep launch-hybrid-job\n",
      "ubuntu    132028  0.0  0.0   8176   724 pts/0    S+   06:14   0:00 grep launch-hybrid-job\n"
     ]
    }
   ],
   "source": [
    "# run the following scripts to check the created threads\n",
    "!ps -aux | grep launch-hybrid-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493a026-3e3f-4b6c-b40c-f0cbf6c4d53b",
   "metadata": {},
   "source": [
    "# Step 4: Jobs finish and visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3c644-4caa-43ac-99b6-6fd63674a882",
   "metadata": {},
   "source": [
    "Please use the following code to check the status of hybrid jobs. The status of hybrid jobs can also be checked in the Amazon Braket console. Optionally, if the email if input when deploying the solution, emails will be sent at the same number of hybrid jobs once \n",
    "the status of jobs changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a643773a-ecaa-4af5-a7eb-ed276f7930b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state of job rna-folding-PKP--10-ml-m5-large-1673244331 is : COMPLETED\n",
      "the state of job rna-folding-PKP--10-ml-m5-4large-1673244336 is : COMPLETED\n",
      "all jobs completed\n"
     ]
    }
   ],
   "source": [
    "# run the following code to test whether all the jobs finish\n",
    "results = []\n",
    "if os.path.exists(hybrid_jobs_json):\n",
    "    # recover hybrid jobs and show result\n",
    "    jobs_states_load = None\n",
    "    with open(hybrid_jobs_json, \"r\") as outfile:\n",
    "        jobs_states_load = json.load(outfile)\n",
    "\n",
    "    completed_jobs_arn = set()\n",
    "\n",
    "    for job_name, job_arn in zip(jobs_states_load[\"names\"], jobs_states_load[\"hybrid-jobs-arn\"]):\n",
    "        current_job = AwsQuantumJob(job_arn)\n",
    "        print(f\"the state of job {job_name} is : {current_job.state()}\")\n",
    "        if current_job.state() == 'COMPLETED':\n",
    "            completed_jobs_arn.update({job_arn})\n",
    "\n",
    "    whole_jobs_num = len(jobs_states_load[\"names\"])\n",
    "\n",
    "    if len(completed_jobs_arn) == whole_jobs_num:\n",
    "        print(f\"all jobs completed\")\n",
    "        for job_arn in completed_jobs_arn:\n",
    "            current_job = AwsQuantumJob(job_arn)\n",
    "            results.append(current_job.result())\n",
    "        # display results\n",
    "        results = display_results(results, experiments_params)\n",
    "else:\n",
    "    print(f\"JSON file for job arns not generated! please wait for the thread(launch-hybrid-job) to finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9924cb0d-be27-49be-a8f0-e466648e269a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f34ec96da6954a5cb31d24600df3f844\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f34ec96da6954a5cb31d24600df3f844\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f34ec96da6954a5cb31d24600df3f844\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3c3ea68c6533ff6d72d3fa4540763fbf\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"field\": \"Device\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Sequence Length\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time to Solution\", \"type\": \"quantitative\"}}, \"height\": 600, \"selection\": {\"selector004\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"rna-folding experiments\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-3c3ea68c6533ff6d72d3fa4540763fbf\": [{\"Sequence Length\": 19, \"Time to Solution\": 2.9546620845794678, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 23, \"Time to Solution\": 4.3616783618927, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 26, \"Time to Solution\": 2.335326671600342, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 27, \"Time to Solution\": 7.954500198364258, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 47, \"Time to Solution\": 9.59642505645752, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 60, \"Time to Solution\": 49.05866098403931, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 84, \"Time to Solution\": 110.08129668235779, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 87, \"Time to Solution\": 203.3412845134735, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 97, \"Time to Solution\": 98.20115923881531, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 122, \"Time to Solution\": 962.4519248008728, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 150, \"Time to Solution\": 1186.007441520691, \"Device\": \"ml.m5.large\"}, {\"Sequence Length\": 19, \"Time to Solution\": 2.868117570877075, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 23, \"Time to Solution\": 4.262963533401489, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 26, \"Time to Solution\": 2.2763524055480957, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 27, \"Time to Solution\": 6.716038227081299, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 47, \"Time to Solution\": 9.407540559768677, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 60, \"Time to Solution\": 47.74212718009949, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 84, \"Time to Solution\": 104.61903238296509, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 87, \"Time to Solution\": 192.3392572402954, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 97, \"Time to Solution\": 93.01825642585754, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 122, \"Time to Solution\": 900.8668532371521, \"Device\": \"ml.m5.4xlarge\"}, {\"Sequence Length\": 150, \"Time to Solution\": 3222.873341321945, \"Device\": \"ml.m5.4xlarge\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename_result = {}\n",
    "device_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "for k,vs in results.items():\n",
    "    k = k.replace(\"\\'\",\"\\\"\")\n",
    "    dict_k = json.loads(k)\n",
    "    device_name = None\n",
    "    if dict_k['qc'] == 'null':\n",
    "        device_name = dict_k['cc']\n",
    "    else:\n",
    "        device_name = dict_k['qc']\n",
    "    for v in vs:\n",
    "        device_list.append(device_name)\n",
    "        x_list.append(v[0])\n",
    "        y_list.append(v[1])\n",
    "source = pd.DataFrame({\n",
    "    \"Sequence Length\": np.array(x_list),\n",
    "    \"Time to Solution\": np.array(y_list),\n",
    "    \"Device\": np.array(device_list),\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_line(point = True).encode(\n",
    "    x='Sequence Length',\n",
    "    y='Time to Solution',\n",
    "    color='Device',\n",
    ").properties(\n",
    "    title = f\"{experiment_name} experiments\",\n",
    "    width = 700,\n",
    "    height = 600,\n",
    ").interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

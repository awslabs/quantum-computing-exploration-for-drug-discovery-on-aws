{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0531faa8-9b68-4a7f-833a-b18efbac1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.aws import AwsDevice\n",
    "from braket.aws import AwsQuantumJob, AwsSession\n",
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6283c75a-3c8b-4a9e-9fd8-709e7d8da56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "c = copy.deepcopy\n",
    "\n",
    "def obtain_default_bucket(target: str) -> str:\n",
    "    with open(os.path.abspath('../../..') + '/.default-setting') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if (line.startswith(target+'=')):\n",
    "                return line.split('=')[1].strip()\n",
    "            \n",
    "def get_key(single_dict):\n",
    "    for k in single_dict.keys():\n",
    "        return k\n",
    "\n",
    "def parse_params(params_list, hp, hp_list):\n",
    "    params = params_list[0]\n",
    "    k = get_key(params)\n",
    "    ps = params[k]\n",
    "    for p in ps:\n",
    "        hp[k] = p\n",
    "        if len(params_list) == 1:\n",
    "            hp_list.append(c(hp))\n",
    "        else:\n",
    "            parse_params(params_list[1:], hp, hp_list)\n",
    "\n",
    "def get_quantum_device(device_name):\n",
    "    device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "    try:\n",
    "        device = AwsDevice.get_devices(names=[device_name])\n",
    "        device_arn = device[0].arn\n",
    "    except Exception as e:\n",
    "        print(f\"fail to get {device_name}: {e}, use sv1 instead\")\n",
    "    return device_arn\n",
    "\n",
    "# def upload_data(dir_name, suffix_check, aws_session=AwsSession()):\n",
    "\n",
    "#     stream_s3_uri = aws_session.construct_s3_uri(obtain_default_bucket(\"bucketName\"), dir_name)\n",
    "#     return_path = None\n",
    "    \n",
    "#     def _check_upload(file_name, check_list):\n",
    "#         file_end = file_name.split('.')[-1]\n",
    "#         print(f\"file end is {file_end}\")\n",
    "#         if file_end in check_list:\n",
    "#             path = f\"{stream_s3_uri}/\" + file_name.split('/')[-1]\n",
    "#             aws_session.upload_to_s3(file_name, path)\n",
    "            \n",
    "#     if os.path.isdir(dir_name):\n",
    "#         dir_list = os.listdir(dir_name)\n",
    "#         print(f\"!! dir_list {dir_list}\")\n",
    "#         for file_name in dir_list:\n",
    "#             _check_upload(os.path.join(dir_name,file_name), suffix_check)\n",
    "#         return_path = stream_s3_uri\n",
    "#     else:\n",
    "#         _check_upload(file_name, suffix_check)\n",
    "#         single_file_name = file_name.split('/')[-1]\n",
    "#         return_path = f\"{stream_s3_uri}/{single_file_name}\"\n",
    "def upload_data(dir_name):\n",
    "    \n",
    "    bucket_name = obtain_default_bucket(\"bucketName\")\n",
    "    \n",
    "    # create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # sync the local folder to the S3 bucket\n",
    "    for root, dirs, files in os.walk(dir_name):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            s3.upload_file(local_path, bucket_name, local_path)\n",
    "\n",
    "    return_path = f\"s3://{bucket_name}/{dir_name}\"\n",
    "        \n",
    "    return return_path\n",
    "\n",
    "\n",
    "\n",
    "def queue_check(jobs):\n",
    "    # check current running or queued jobs\n",
    "    region = boto3.client('s3').meta.region_name\n",
    "    braket = boto3.client(\"braket\", region_name=region)\n",
    "    p = braket.get_paginator(\"search_jobs\")\n",
    "    paginator = p.paginate(filters=[]).build_full_result()\n",
    "    \n",
    "    existing_job_count = 0\n",
    "    if \"jobs\" in paginator:\n",
    "        for job in paginator[\"jobs\"]:\n",
    "            if job[\"status\"] in [\"RUNNING\", \"QUEUED\"]:\n",
    "                existing_job_count = existing_job_count + 1\n",
    "    \n",
    "    check_pass = True\n",
    "    if existing_job_count >= 4:\n",
    "        check_pass = False\n",
    "    \n",
    "    print(f\"There are {existing_job_count} jobs in RUNNING or QUEUED status\")\n",
    "    \n",
    "    return check_pass\n",
    "\n",
    "def get_result(result, target, dm):\n",
    "    return [dm, result[\"time\"], target]\n",
    "\n",
    "def get_dm(target):\n",
    "    file_name = './rna-folding-data/' + target + '.fasta.txt'\n",
    "    str_len = -100\n",
    "    with open(file_name) as file:\n",
    "        fasta_lines = file.readlines()\n",
    "        str_len = len(fasta_lines[1])\n",
    "    return str_len\n",
    "\n",
    "def display_results(results, experiments_params):\n",
    "    sorted_results = {}\n",
    "    \n",
    "    for device in experiments_params[\"params\"][4][\"device\"]:\n",
    "        sorted_results[str(device)] = []\n",
    "        # for target in results[0].keys():\n",
    "        #     sorted_results[str(device)][target] = []\n",
    "    \n",
    "    max_offset = 10\n",
    "    for result in results:\n",
    "        for target in result.keys():\n",
    "            device = result[target][\"hypermeter\"][\"device\"]\n",
    "            \n",
    "            dm = get_dm(target)\n",
    "\n",
    "            if len(sorted_results[device]) == 0:\n",
    "                sorted_results[device].append(get_result(result[target],target,dm))\n",
    "                continue\n",
    "\n",
    "            last_idx = 0\n",
    "            for idx, sorted_result in enumerate(sorted_results[device]):\n",
    "                sorted_dm = float(sorted_result[0])\n",
    "\n",
    "                last_sorted_dm = float(sorted_results[device][last_idx][0])\n",
    "\n",
    "                if last_sorted_dm < dm and dm < sorted_dm:\n",
    "                    sorted_results[device].insert(idx,get_result(result[target],target,dm))\n",
    "                    break\n",
    "                elif dm < last_sorted_dm and idx == 0:\n",
    "                    sorted_results[device].insert(last_idx,get_result(result[target],target,dm))\n",
    "                    break\n",
    "                elif dm > sorted_dm and idx == len(sorted_results[device])-1:\n",
    "                    sorted_results[device].insert(idx+1,get_result(result[target],target,dm))\n",
    "\n",
    "                last_idx = idx\n",
    "    # print(f\"sorted result {sorted_results}\")\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69851056-a6fb-46f9-9915-4ebdd0b300d9",
   "metadata": {},
   "source": [
    "# Step 1: Prepare parameters for batch evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933ed89-1906-4b4f-9d5e-b6af78a28cea",
   "metadata": {},
   "source": [
    "In this part, we set the parameters for batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2cc57f-b61f-48ef-8d7b-580f47f66d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for experiments: \n",
      " [{'method': 'qfold-cc', 'initialization': 'minifold', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.large'}}, {'method': 'qfold-cc', 'initialization': 'minifold', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.4xlarge'}}, {'method': 'qfold-cc', 'initialization': 'random', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.large'}}, {'method': 'qfold-cc', 'initialization': 'random', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.4xlarge'}}, {'method': 'qfold-qc', 'initialization': 'minifold', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.large'}}, {'method': 'qfold-qc', 'initialization': 'minifold', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.4xlarge'}}, {'method': 'qfold-qc', 'initialization': 'random', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.large'}}, {'method': 'qfold-qc', 'initialization': 'random', 'shots': 10000, 'mode': 'local-simulator', 'device': {'qc': 'null', 'cc': 'ml.m5.4xlarge'}}]\n"
     ]
    }
   ],
   "source": [
    "# parameters for experiments\n",
    "experiment_name = \"QfoldHybridJobs\"\n",
    "data_path = \"protein-folding-data\"\n",
    "suffix_check = [\"json\"]\n",
    "experiments_params =  {\n",
    "    \"version\": \"1\",\n",
    "    \"params\": [\n",
    "        {\"method\": [\"qfold-cc\", \"qfold-qc\"]},\n",
    "        {\"initialization\": [\"minifold\", \"random\"]},\n",
    "        {\"shots\": [10000]},\n",
    "        {\"mode\": [\"local-simulator\"]},\n",
    "        {\"device\": [{\"qc\": \"null\", \"cc\": \"ml.m5.large\"},{\"qc\": \"null\", \"cc\": \"ml.m5.4xlarge\"}]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "hp = {}\n",
    "hybrid_job_params = []\n",
    "parse_params(experiments_params['params'], hp, hybrid_job_params)\n",
    "\n",
    "print(f\"parameters for experiments: \\n {hybrid_job_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2062d7b-b958-4c35-bf9f-202480d4d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload data to s3 path: s3://amazon-braket-us-east-1-002224604296/protein-folding-data\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to S3\n",
    "s3_path = upload_data(data_path)\n",
    "print(f\"upload data to s3 path: {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed41e7-fbb4-4c16-b375-e7ef94754e12",
   "metadata": {},
   "source": [
    "# Step 2: Prepare image for experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabecc36-a574-4a7c-b9bf-94af53dcb086",
   "metadata": {},
   "source": [
    "In this part, we use the following code to prepare the image for experiment. For the first run, \n",
    "please run build_and_push.sh to create the image. For future experiments, avoid running\n",
    "build_and_push.sh unless you want to rebuild the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f2bf0f-47ab-4e44-adb7-474cd7e86464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/ubuntu/psi4conda/bin/psi4 QfoldHybridJobs/psi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "320b1c6b-c575-4d41-a69a-cde747d903cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hybrid job image for 002224604296 in region us-east-1: 002224604296.dkr.ecr.us-east-1.amazonaws.com/amazon-braket-qfoldhybridjobs-jobs:latest\n"
     ]
    }
   ],
   "source": [
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.client('s3').meta.region_name\n",
    "image_name = f\"amazon-braket-{experiment_name.lower()}-jobs\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "\n",
    "print(f\"the hybrid job image for {account_id} in region {region}: {image_uri}\")\n",
    "\n",
    "# For the first run, please use the following code to create the image for this application. For future experiments, comment\n",
    "# the following code unless you want to rebuild the image\n",
    "# !sh build_and_push.sh {image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87c7a433-6518-4f13-8b0f-4abd35b4c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job info will be saved in QfoldHybridJobs-hybrid-jobs.json\n"
     ]
    }
   ],
   "source": [
    "hybrid_jobs_json = f\"{experiment_name}-hybrid-jobs.json\"\n",
    "print(f\"job info will be saved in {hybrid_jobs_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4c8d9-8db8-443e-992f-bf344b051eff",
   "metadata": {},
   "source": [
    "# Step 3: Launch Amazon Braket Hybrid Jobs for experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a1eb3-bb7f-4f53-804e-fec3db66c1bc",
   "metadata": {},
   "source": [
    "In this part, we use the following code to launch the same number of hybrid jobs as the sets of parameters for this experiments.\n",
    "When the number of jobs exceeds 5 RPS, this thread will wait. The default setting of this experiment will take around **7 hours** to \n",
    "finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1d52f18-791a-4943-abaf-1a9911815709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'QfoldHybridJobs-hybrid-jobs.json': No such file or directory\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-m-l-ml-m5-large-1679454022\n",
      "Finish create QfoldHybridJobs with q-m-l-ml-m5-large-1679454022\n",
      "There are 1 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-m-l-ml-m5-4large-1679454037\n",
      "Finish create QfoldHybridJobs with q-m-l-ml-m5-4large-1679454037\n",
      "There are 2 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-r-l-ml-m5-large-1679454044\n",
      "Finish create QfoldHybridJobs with q-r-l-ml-m5-large-1679454044\n",
      "There are 3 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-r-l-ml-m5-4large-1679454050\n",
      "Finish create QfoldHybridJobs with q-r-l-ml-m5-4large-1679454050\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 3 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-m-l-ml-m5-large-1679454236\n",
      "Finish create QfoldHybridJobs with q-m-l-ml-m5-large-1679454236\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 3 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-m-l-ml-m5-4large-1679454268\n",
      "Finish create QfoldHybridJobs with q-m-l-ml-m5-4large-1679454268\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 3 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-r-l-ml-m5-large-1679454314\n",
      "Finish create QfoldHybridJobs with q-r-l-ml-m5-large-1679454314\n",
      "There are 3 jobs in RUNNING or QUEUED status\n",
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is q-r-l-ml-m5-4large-1679454322\n",
      "Finish create QfoldHybridJobs with q-r-l-ml-m5-4large-1679454322\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 4 jobs in RUNNING or QUEUED status\n",
      "There are 3 jobs in RUNNING or QUEUED status\n",
      "Finish launch all the hybrid jobs and save all the files\n"
     ]
    }
   ],
   "source": [
    "# Long runnning cell due to Burst rate of CreateJob requests < 5 RPS\n",
    "# sudo apt-get install python-prctl at first\n",
    "# https://stackoverflow.com/questions/34361035/python-thread-name-doesnt-show-up-on-ps-or-htop\n",
    "from threading import Thread\n",
    "import threading\n",
    "import setproctitle\n",
    "\n",
    "def launch_hybrid_jobs(hybrid_job_params=hybrid_job_params, hybrid_jobs_json=hybrid_jobs_json):\n",
    "    setproctitle.setproctitle(threading.current_thread().name)\n",
    "    # parse evaluation parameters and trigger hybrid jobs:\n",
    "    jobs = []\n",
    "    names = []\n",
    "\n",
    "    job_name = experiment_name.lower()\n",
    "    device_param_list = [\"shots\", \"device\"]\n",
    "\n",
    "    for job_param in hybrid_job_params:\n",
    "        \n",
    "        algorithm_param_name = \"\"\n",
    "        for k,v in job_param.items():\n",
    "            if k not in device_param_list:\n",
    "                algorithm_param_name = algorithm_param_name+f\"-{v[0]}\"\n",
    "        algorithm_param_name=algorithm_param_name[1:]\n",
    "        quantum_device = get_quantum_device(job_param['device']['qc'])\n",
    "        classical_device = job_param['device']['cc']\n",
    "\n",
    "        device_name = classical_device.replace(\".\",\"-\")\n",
    "        device_name = device_name.replace(\"x\",\"\")\n",
    "        \n",
    "        name = f\"{algorithm_param_name}-{device_name}-\" + str(int(time.time()))\n",
    "        name = name.lower()\n",
    "        # name = f\"{experiment_name}-\"+ str(int(time.time()))\n",
    "        print(f\"name is {name}\")\n",
    "\n",
    "        tmp_job = AwsQuantumJob.create(\n",
    "            device=quantum_device,\n",
    "            source_module=f\"{experiment_name}\",\n",
    "            entry_point=f\"{experiment_name}.{job_name}:main\",\n",
    "            job_name=name,\n",
    "            hyperparameters=job_param,\n",
    "            input_data=s3_path,\n",
    "            instance_config=InstanceConfig(instanceType=classical_device),\n",
    "            image_uri=image_uri,\n",
    "            wait_until_complete=False,\n",
    "        )\n",
    "        \n",
    "#         from braket.jobs.local import LocalQuantumJob\n",
    "        \n",
    "#         tmp_job = LocalQuantumJob.create(\n",
    "#             device=quantum_device,\n",
    "#             source_module=f\"{experiment_name}\",\n",
    "#             entry_point=f\"{experiment_name}.{job_name}:main\",\n",
    "#             hyperparameters=job_param,\n",
    "#             input_data=s3_path,\n",
    "#             image_uri=image_uri,\n",
    "#         )   \n",
    "        \n",
    "        print(f\"Finish create {experiment_name} with {name}\")\n",
    "\n",
    "        jobs.append(tmp_job)\n",
    "        names.append(name)\n",
    "\n",
    "\n",
    "        while not queue_check(jobs):\n",
    "            time.sleep(5)\n",
    "    jobs_arn = []\n",
    "\n",
    "    for job in jobs:\n",
    "        jobs_arn.append(job.arn)\n",
    "\n",
    "    jobs_states = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"hybrid-jobs-arn\": jobs_arn,\n",
    "        \"names\": names\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # save hybrid job arn for further analysis\n",
    "    json_object = json.dumps(jobs_states, indent=4)\n",
    "\n",
    "    with open(hybrid_jobs_json, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "        \n",
    "    print(f\"Finish launch all the hybrid jobs and save all the files\")\n",
    "\n",
    "# remove existing hybrid_jobs_json file\n",
    "!rm {hybrid_jobs_json}\n",
    "\n",
    "t = Thread(target=launch_hybrid_jobs, name=\"launch-hybrid-job\", daemon=True).start()\n",
    "\n",
    "# launch_hybrid_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1b433d-440a-42ed-8cf2-fc6b48ea97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu     52285  0.7  1.0 2466724 343908 ?      Ssl  02:42   0:16 launch-hybrid-job\n",
      "ubuntu     53803  0.0  0.0   8756  3432 pts/21   Ss+  03:20   0:00 /bin/bash -c ps -aux | grep launch-hybrid-job\n",
      "ubuntu     53808  0.0  0.0   8176   728 pts/21   S+   03:20   0:00 grep launch-hybrid-job\n"
     ]
    }
   ],
   "source": [
    "# run the following scripts to check the created threads\n",
    "!ps -aux | grep launch-hybrid-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493a026-3e3f-4b6c-b40c-f0cbf6c4d53b",
   "metadata": {},
   "source": [
    "# Step 4: Jobs finish and visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3c644-4caa-43ac-99b6-6fd63674a882",
   "metadata": {},
   "source": [
    "Please use the following code to check the status of hybrid jobs. The status of hybrid jobs can also be checked in the Amazon Braket console. Optionally, if the email if input when deploying the solution, emails will be sent at the same number of hybrid jobs once \n",
    "the status of jobs changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a643773a-ecaa-4af5-a7eb-ed276f7930b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state of job q-m-l-ml-m5-large-1679454022 is : COMPLETED\n",
      "the state of job q-m-l-ml-m5-4large-1679454037 is : COMPLETED\n",
      "the state of job q-r-l-ml-m5-large-1679454044 is : COMPLETED\n",
      "the state of job q-r-l-ml-m5-4large-1679454050 is : COMPLETED\n",
      "the state of job q-m-l-ml-m5-large-1679454236 is : COMPLETED\n",
      "the state of job q-m-l-ml-m5-4large-1679454268 is : COMPLETED\n",
      "the state of job q-r-l-ml-m5-large-1679454314 is : COMPLETED\n",
      "the state of job q-r-l-ml-m5-4large-1679454322 is : COMPLETED\n",
      "all jobs completed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hypermeter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3e01f4d6d402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiments_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"JSON file for job arns not generated! please wait for the thread(launch-hybrid-job) to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-aa9b9d2db559>\u001b[0m in \u001b[0;36mdisplay_results\u001b[0;34m(results, experiments_params)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hypermeter\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hypermeter'"
     ]
    }
   ],
   "source": [
    "# run the following code to test whether all the jobs finish\n",
    "results = []\n",
    "if os.path.exists(hybrid_jobs_json):\n",
    "    # recover hybrid jobs and show result\n",
    "    jobs_states_load = None\n",
    "    with open(hybrid_jobs_json, \"r\") as outfile:\n",
    "        jobs_states_load = json.load(outfile)\n",
    "\n",
    "    completed_jobs_arn = set()\n",
    "\n",
    "    for job_name, job_arn in zip(jobs_states_load[\"names\"], jobs_states_load[\"hybrid-jobs-arn\"]):\n",
    "        current_job = AwsQuantumJob(job_arn)\n",
    "        print(f\"the state of job {job_name} is : {current_job.state()}\")\n",
    "        if current_job.state() == 'COMPLETED':\n",
    "            completed_jobs_arn.update({job_arn})\n",
    "\n",
    "    whole_jobs_num = len(jobs_states_load[\"names\"])\n",
    "\n",
    "    if len(completed_jobs_arn) == whole_jobs_num:\n",
    "        print(f\"all jobs completed\")\n",
    "        for job_arn in completed_jobs_arn:\n",
    "            current_job = AwsQuantumJob(job_arn)\n",
    "            results.append(current_job.result())\n",
    "        # display results\n",
    "        results = display_results(results, experiments_params)\n",
    "else:\n",
    "    print(f\"JSON file for job arns not generated! please wait for the thread(launch-hybrid-job) to finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9924cb0d-be27-49be-a8f0-e466648e269a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d520545c3318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdict_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail to get null: list index out of range, use sv1 instead\n",
      "name is qfold-cc-minifold-local-simulator-ml-m5-large-1679452955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread launch-hybrid-job:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qfold-python3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qfold-python3.7/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-13347a113ad7>\", line 44, in launch_hybrid_jobs\n",
      "    wait_until_complete=False,\n",
      "  File \"/opt/conda/envs/qfold-python3.7/lib/python3.7/site-packages/braket/aws/aws_quantum_job.py\", line 198, in create\n",
      "    job_arn = aws_session.create_job(**create_job_kwargs)\n",
      "  File \"/opt/conda/envs/qfold-python3.7/lib/python3.7/site-packages/braket/aws/aws_session.py\", line 244, in create_job\n",
      "    response = self.braket_client.create_job(**boto3_kwargs)\n",
      "  File \"/opt/conda/envs/qfold-python3.7/lib/python3.7/site-packages/botocore/client.py\", line 508, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/opt/conda/envs/qfold-python3.7/lib/python3.7/site-packages/botocore/client.py\", line 911, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (BadRequestException) when calling the CreateJob operation: Invalid request body\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rename_result = {}\n",
    "device_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "for k,vs in results.items():\n",
    "    k = k.replace(\"\\'\",\"\\\"\")\n",
    "    dict_k = json.loads(k)\n",
    "    device_name = None\n",
    "    if dict_k['qc'] == 'null':\n",
    "        device_name = dict_k['cc']\n",
    "    else:\n",
    "        device_name = dict_k['qc']\n",
    "    for v in vs:\n",
    "        device_list.append(device_name)\n",
    "        x_list.append(v[0])\n",
    "        y_list.append(v[1])\n",
    "source = pd.DataFrame({\n",
    "    \"Sequence Length\": np.array(x_list),\n",
    "    \"Time to Solution\": np.array(y_list),\n",
    "    \"Device\": np.array(device_list),\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_line(point = True).encode(\n",
    "    x='Sequence Length',\n",
    "    y='Time to Solution',\n",
    "    color='Device',\n",
    ").properties(\n",
    "    title = f\"{experiment_name} experiments\",\n",
    "    width = 700,\n",
    "    height = 600,\n",
    ").interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
